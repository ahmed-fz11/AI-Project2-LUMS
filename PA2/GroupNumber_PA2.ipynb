{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to AI (CS331): Project 2\n",
    "#### Name: \n",
    "#### Roll Number: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import svm, datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "## Dataset [5 marks]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "Let's start by loading the dataset. For this part, we will use the `bike_hour.csv`. Load the daataset into a dataframe using the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "df = pd.read_csv('bike_hour.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Dataset\n",
    "Make sure there are no NaN or null values in the dataset. If there are any, remove those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression by Least Squares [20 marks]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to use pseudo-inverse, bias error and variance error, to find the best fit equation for our dataset. We are going to make use of the equation:\n",
    "$$\n",
    "    w = (A^TA)^{-1}A^Ty\n",
    "$$\n",
    "We already have our dataset. For this question, we are going to focus on univariate polynomial regression, so lets select on feature from our dataset that we are going to use and split it into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the code to extract the number of bikes rented and the \"atemp\" feature from the dataset.\n",
    "# Do a 70 30 split to make test and train datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, use it to make a graph of variance loss (test loss) and bias loss (training loss), using the equation given above. Do this for polynomials 1: ($y = a+bx$) till 20 ($y = a+  bx + cx^2 + ... + ux^{20}$) The reuslting graph you get should resemeble the one shown in the manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Which polynomial is most optimal to use for this dataset? Explain your choice and mention both bias and variance losses for this polynomial. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER here:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression by Gradient Descent [15 marks]\n",
    "You are now going to implement linear regression using gradient descent. Unlike in the above part, your regression is now linerar but should work for any number of variables (multivariate liner regression). For this part, start by using the complete dataframe with all teh features instead of just one feature, as we did in the above part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to load the complete dataframe again or call it again, do so here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by removing any features (columns), which will not be useful for us. From the complete dataframe, remove the columns that you think are not going to be useful for regression. In the markdown cell, **EXPLAIN  your choice** behind removing the columns that you did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER here: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "Similar to as you already did, make an 70 30 split to divide the dataset into training and test dataset. If you want, you can use inbuilt libraries for this.\n",
    "Also divide the datasets into features and labels, which will results in 4 variables: X_train, Y_train, X_test, Y_test (you can use different names if you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Complete the following functions to run linear regression. \"y_pred\" represents the array of predicted target values that we will get while \"y_true\" represents the array of labels that we are already provided with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_pred, y_true): # y_pred is the array of the values we get after we multiply weights with the attributes ( h(x) ), and y_true is an array of true labels\n",
    "    pass\n",
    "    # code here\n",
    "    \n",
    "\n",
    "\n",
    "def gradient_des(x, thetas, y_pred, y_true, alpha):\n",
    "    pass\n",
    "    # code here and return both thetas and the losses\n",
    "    # returning both is important for visualising the losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "For the specified number of epochs, run the regression and store the losses and corresponding thetas for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 500  # you are free to play around with this number\n",
    "theta = ... # initialize an np array of zeros based on the shape of the training sample\n",
    "\n",
    "losses = []\n",
    "\n",
    "# code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the losses\n",
    "You can run the following cell to see how your losses change with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(list(range(len(losses))), losses)\n",
    "plt.title(\"Loss Plot\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "Now use the final thetas that you got to find the accuracy of the regressor on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset [5 marks]\n",
    "For this part, we will use the iris dataset provided by sklearn. The dataset library has already been imported in first cell. Use this library to import the iris dataset along with its labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and changing the dataset\n",
    "There are two tasks that you have to perform in this task:\n",
    "1. This dataset contains 3 classes of iris. Irir setosa is linearly seperable from the other two. In you target (labels) array, you have three label values: 0, 1 and 2. Using the information given in the iris dataset, identify which one of these represents Iris Setosa and chage your target values such that instead of having 3 label values, you only have two, where 0 would represent the class Iris Setosa while 1 would mean that the class is not Iris Setosa. \n",
    "2. Now that we have the features and the labels extracted, once again make test and train datasets for logistic regression using a 70 30 split. Make sure that the data is properly shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression by Scratch [15 marks]\n",
    "We are now going to perform logistic regression by scratch on our training dataset. Complete the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y_pred, y_true):\n",
    "    pass\n",
    "    # code here\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    pass\n",
    "    # code here\n",
    "\n",
    "\n",
    "def gradient_des(x, thetas, h_x, y, alpha):\n",
    "    pass\n",
    "    # code here and return both thetas and the losses\n",
    "    # returning both is important for visualising the losses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "For the specified number of epochs, run the logistic regression and store the losses and corresponding thetas for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100  # you are free to play around with this number\n",
    "theta =  ... # initialize an np array of zeros based on the shape of the training sample\n",
    "\n",
    "losses = []\n",
    "\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the losses\n",
    "You can run the following cell to see how your losses change with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(losses))), losses)\n",
    "plt.title(\"Loss against Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation [15 marks]\n",
    "As you might have noticed, our iris training set is not that big (with only 105 values). To make our model perform better, we are going to implement k-fold cross-validation.\n",
    "\n",
    "In k cross-validation, you divide your training set into k parts and use k-1 of them for training, while one part is used for validation. We then rerun this k times, where each time, a different part from the total of k parts is used for validation. Note that the test data set still remains completely seperate.\n",
    "\n",
    "This will result in you having k different weights (one for each fold). In the end, take na average of all the weights and those will be your final weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5 # we are going to keep k as 5 for this example but your code should not hardocde this value\n",
    "# code here the k-fold cross validation for the iris training dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the final averaged weights on the test data and report the Binary Cross Entroly Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "Did the cross-validation help in making the model better? Why or why not?\n",
    "\n",
    "### ANSWER here:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using Libraries [5 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as sl\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the above provided libraries to implement [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and use the [Cross Entropy Loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) to report loss on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMs [20 marks]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to use the SVM library by sklearn on the iris dataset we used for logistic regression. You can either use the same variables, or make a new cell here and import the datset again. However, for the SVM classifier, we are going to extract only the first two features, `Sepal length` and `Sepal width`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here to get the iris datset with labels and only the first two features (\"Sepal length\" and \"Sepal width\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Visualisation\n",
    "Train the SVM using the iris dataset and make a plot similar to the one shown in the manual. To make the plot, you are going to have to look into the following functions:\n",
    "- np.meshgrid\n",
    "- plt.scatter\n",
    "- plt.contourf\n",
    "\n",
    "However, you are free to use any functions and libraries that you want to use.\n",
    "\n",
    "Make three different plits for the following three kernels of SVMs: linear, poly, and rbf.\n",
    "You are free to play around with SVM parameters like C (regulization parameter) and gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\"linear\", \"poly\", \"rbf\"]\n",
    "# code here\n",
    "'''\n",
    "1. Use the dataset to train the SVM\n",
    "2. Create a mesh\n",
    "3. Get the predicted values\n",
    "4. Plot the graph\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the SVMs\n",
    "Using the train-test split of 70-30, find the accuracies for all three SVMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "Which kernel gave the highest accuracy? Why is that the case?\n",
    "### ANSWER here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
